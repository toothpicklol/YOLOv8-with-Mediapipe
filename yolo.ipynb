{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toothpicklol/YOLOv8-with-Mediapipe/blob/main/yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install mediapipe"
      ],
      "metadata": {
        "id": "31chCxVBnCHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install ultralytics"
      ],
      "metadata": {
        "id": "CiQTf7EonFxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates\n",
        "import mediapipe  as mp\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import time"
      ],
      "metadata": {
        "id": "hwQ1LBB6mJKg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_drawing = mp.solutions.drawing_utils         # mediapipe drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles # mediapipe drawing_styles\n",
        "mp_holistic = mp.solutions.holistic\n",
        "_PRESENCE_THRESHOLD = 0.5\n",
        "_VISIBILITY_THRESHOLD = 0.5\n",
        "model = YOLO(\"sample_data/yolov8n.pt\")\n",
        "gif_file = 'sample_data/walk.gif'\n",
        "cap = cv2.VideoCapture(gif_file)\n",
        "count=0"
      ],
      "metadata": {
        "id": "YhWNoZVim9cx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mp_holistic.Holistic(\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5) as holistic:\n",
        "    while True:\n",
        "        start = time.time()\n",
        "\n",
        "        # Read a frame from the GIF\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Check if the frame was read successfully\n",
        "        if not ret:\n",
        "            break\n",
        "        # im2 = cv2.imread(\"man.jpg\")\n",
        "        im2=frame\n",
        "        final = im2\n",
        "        results = model(source=im2, classes=0)  # classes =0 only check type = \"person\"  #save_crop=True, save_txt=True,\n",
        "        end = time.time()\n",
        "        # 輸出結果\n",
        "        print(\"yolo執行時間：%f 秒\" % (end - start))\n",
        "\n",
        "        box_arr = []  # yolo\n",
        "        # loc_array=[]  # original loc with yolo\n",
        "        mp_array = []  # mediapipe loc with cut by original img need change\n",
        "\n",
        "        # get yolo results loc\n",
        "        for i in results[0].boxes.xywhn:\n",
        "            box_arr.append(f\"{i[0].item()} {i[1].item()} {i[2].item()} {i[3].item()}\")\n",
        "\n",
        "        dh, dw, _ = im2.shape\n",
        "        for i in box_arr:\n",
        "\n",
        "            # transform to original img loc\n",
        "\n",
        "            x_center, y_center, w, h = i.strip().split()\n",
        "            x_center, y_center, w, h = float(x_center), float(y_center), float(w), float(h)\n",
        "            x_center = round(x_center * dw)\n",
        "            y_center = round(y_center * dh)\n",
        "\n",
        "            w = round(w * dw)\n",
        "            h = round(h * dh)\n",
        "            x = round(x_center - w / 2)\n",
        "            y = round(y_center - h / 2)\n",
        "            #  crop img and append loc\n",
        "            # loc_array.append([w,h,x,y])\n",
        "            imgCrop = im2[y:y + h, x:x + w]\n",
        "\n",
        "            #  drawing mediapipe\n",
        "            frame = imgCrop\n",
        "            img = frame\n",
        "            start = time.time()\n",
        "            img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 將 BGR 轉換成 RGB\n",
        "            results = holistic.process(img2)  # 開始偵測全身\n",
        "            mp_drawing.draw_landmarks(\n",
        "                img,\n",
        "                results.pose_landmarks,\n",
        "                mp_holistic.POSE_CONNECTIONS,\n",
        "                landmark_drawing_spec=mp_drawing_styles\n",
        "                .get_default_pose_landmarks_style())\n",
        "\n",
        "            # Overwrite the original image\n",
        "            final[y:y + h, x:x + w] = img\n",
        "            # cv2.imshow(\"a\", final)\n",
        "            cv2.imwrite(\"sample_data/gif/\"+str(count)+\".jpg\", final)\n",
        "            count += 1\n",
        "            end = time.time()\n",
        "            # 輸出結果\n",
        "            print(\"mediapipe執行時間：%f 秒\" % (end - start))\n",
        "\n",
        "            # get erery pose_landmarks and transform to original img loc\n",
        "            # landmark_list = results.pose_landmarks\n",
        "            # image_rows, image_cols, _ = frame.shape\n",
        "            # idx_to_coordinates = {}\n",
        "\n",
        "            # if landmark_list != None:\n",
        "            #     for idx, landmark in enumerate(landmark_list.landmark):\n",
        "            #         if ((landmark.HasField('visibility') and\n",
        "            #              landmark.visibility < _VISIBILITY_THRESHOLD) or\n",
        "            #                 (landmark.HasField('presence') and\n",
        "            #                  landmark.presence < _PRESENCE_THRESHOLD)):\n",
        "            #             continue\n",
        "            #         landmark_px = _normalized_to_pixel_coordinates(landmark.x, landmark.y, image_cols, image_rows)\n",
        "            #         if landmark_px:\n",
        "            #             idx_to_coordinates[idx] = landmark_px\n",
        "            #     if len(idx_to_coordinates) != 0:\n",
        "            #         mp_array.append(idx_to_coordinates)\n",
        "\n",
        "            #         # transform to original loc\n",
        "            #         for j in idx_to_coordinates:\n",
        "            #             # original x\n",
        "            #             # print(x + idx_to_coordinates.get(j)[0])\n",
        "            #             # original y\n",
        "            #             # print(y + idx_to_coordinates.get(j)[0])\n",
        "            #             print()\n",
        "\n",
        "                    # Because some points cannot be determined,\n",
        "                    # the number of each point has a corresponding position\n",
        "                    # cannot append to array.\n",
        "        # print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Release the video capture object and close any open windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_80PT-JImzbg",
        "outputId": "0745e250-f538-4c9e-8e9d-39ed149ee7f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 117.9ms\n",
            "Speed: 13.4ms preprocess, 117.9ms inference, 39.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：16.548930 秒\n",
            "mediapipe執行時間：0.142117 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 4.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.070750 秒\n",
            "yolo執行時間：0.021062 秒\n",
            "mediapipe執行時間：0.071851 秒\n",
            "mediapipe執行時間：0.070184 秒\n",
            "yolo執行時間：0.020158 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.072869 秒\n",
            "mediapipe執行時間：0.072079 秒\n",
            "yolo執行時間：0.020300 秒\n",
            "mediapipe執行時間：0.073113 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 4.9ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.9ms\n",
            "Speed: 4.0ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.073899 秒\n",
            "yolo執行時間：0.021950 秒\n",
            "mediapipe執行時間：0.069648 秒\n",
            "mediapipe執行時間：0.070311 秒\n",
            "yolo執行時間：0.023271 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.071108 秒\n",
            "mediapipe執行時間：0.085222 秒\n",
            "yolo執行時間：0.021095 秒\n",
            "mediapipe執行時間：0.068084 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 4.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.069177 秒\n",
            "yolo執行時間：0.018905 秒\n",
            "mediapipe執行時間：0.070014 秒\n",
            "mediapipe執行時間：0.068887 秒\n",
            "yolo執行時間：0.022041 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 4.9ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.073491 秒\n",
            "mediapipe執行時間：0.064644 秒\n",
            "yolo執行時間：0.020777 秒\n",
            "mediapipe執行時間：0.069767 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 6.9ms\n",
            "Speed: 2.7ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11.5ms\n",
            "Speed: 2.0ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.070216 秒\n",
            "yolo執行時間：0.022125 秒\n",
            "mediapipe執行時間：0.069672 秒\n",
            "mediapipe執行時間：0.078412 秒\n",
            "yolo執行時間：0.026137 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 6.6ms\n",
            "Speed: 3.8ms preprocess, 6.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.074711 秒\n",
            "mediapipe執行時間：0.074134 秒\n",
            "yolo執行時間：0.019093 秒\n",
            "mediapipe執行時間：0.064396 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 2.0ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.074049 秒\n",
            "yolo執行時間：0.020229 秒\n",
            "mediapipe執行時間：0.067382 秒\n",
            "mediapipe執行時間：0.066928 秒\n",
            "yolo執行時間：0.019230 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 12.2ms\n",
            "Speed: 3.1ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.072867 秒\n",
            "mediapipe執行時間：0.068872 秒\n",
            "yolo執行時間：0.024859 秒\n",
            "mediapipe執行時間：0.075486 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 3 persons, 14.3ms\n",
            "Speed: 2.6ms preprocess, 14.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.106564 秒\n",
            "yolo執行時間：0.036002 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.173889 秒\n",
            "mediapipe執行時間：0.138049 秒\n",
            "mediapipe執行時間：0.048656 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 2 persons, 10.2ms\n",
            "Speed: 8.6ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.044475 秒\n",
            "mediapipe執行時間：0.153171 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 10.5ms\n",
            "Speed: 2.3ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.115697 秒\n",
            "yolo執行時間：0.039586 秒\n",
            "mediapipe執行時間：0.111586 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 38.1ms\n",
            "Speed: 2.3ms preprocess, 38.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.114844 秒\n",
            "yolo執行時間：0.064931 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.170428 秒\n",
            "mediapipe執行時間：0.156511 秒\n",
            "yolo執行時間：0.034981 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.107150 秒\n",
            "mediapipe執行時間：0.109489 秒\n",
            "yolo執行時間：0.037228 秒\n",
            "mediapipe執行時間：0.147428 秒\n",
            "mediapipe執行時間：0.177369 秒\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGFWo9OLTWWVNiUPEsodSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toothpicklol/YOLOv8-with-Mediapipe/blob/main/yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install mediapipe"
      ],
      "metadata": {
        "id": "31chCxVBnCHn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install ultralytics"
      ],
      "metadata": {
        "id": "CiQTf7EonFxC"
      },
      "execution_count": 2,
      "outputs": []
    },    
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates\n",
        "import mediapipe  as mp\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import time"
      ],
      "metadata": {
        "id": "hwQ1LBB6mJKg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_drawing = mp.solutions.drawing_utils         # mediapipe drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles # mediapipe drawing_styles\n",
        "mp_holistic = mp.solutions.holistic\n",
        "_PRESENCE_THRESHOLD = 0.5\n",
        "_VISIBILITY_THRESHOLD = 0.5\n",
        "model = YOLO(\"sample_data/yolov8n.pt\")\n",
        "gif_file = 'sample_data/walk.gif'\n",
        "cap = cv2.VideoCapture(gif_file)\n",
        "count=0"
      ],
      "metadata": {
        "id": "YhWNoZVim9cx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mp_holistic.Holistic(\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5) as holistic:\n",
        "    while True:\n",
        "        start = time.time()\n",
        "\n",
        "        # Read a frame from the GIF\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Check if the frame was read successfully\n",
        "        if not ret:\n",
        "            break\n",
        "        # im2 = cv2.imread(\"man.jpg\")\n",
        "        im2=frame\n",
        "        final = im2\n",
        "        results = model(source=im2, classes=0)  # classes =0 only check type = \"person\"  #save_crop=True, save_txt=True,\n",
        "        end = time.time()\n",
        "        # 輸出結果\n",
        "        print(\"yolo執行時間：%f 秒\" % (end - start))\n",
        "\n",
        "        box_arr = []  # yolo\n",
        "        # loc_array=[]  # original loc with yolo\n",
        "        mp_array = []  # mediapipe loc with cut by original img need change\n",
        "\n",
        "        # get yolo results loc\n",
        "        for i in results[0].boxes.xywhn:\n",
        "            box_arr.append(f\"{i[0].item()} {i[1].item()} {i[2].item()} {i[3].item()}\")\n",
        "\n",
        "        dh, dw, _ = im2.shape\n",
        "        for i in box_arr:\n",
        "\n",
        "            # transform to original img loc\n",
        "\n",
        "            x_center, y_center, w, h = i.strip().split()\n",
        "            x_center, y_center, w, h = float(x_center), float(y_center), float(w), float(h)\n",
        "            x_center = round(x_center * dw)\n",
        "            y_center = round(y_center * dh)\n",
        "\n",
        "            w = round(w * dw)\n",
        "            h = round(h * dh)\n",
        "            x = round(x_center - w / 2)\n",
        "            y = round(y_center - h / 2)\n",
        "            #  crop img and append loc\n",
        "            # loc_array.append([w,h,x,y])\n",
        "            imgCrop = im2[y:y + h, x:x + w]\n",
        "\n",
        "            #  drawing mediapipe\n",
        "            frame = imgCrop\n",
        "            img = frame\n",
        "            img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 將 BGR 轉換成 RGB\n",
        "            results = holistic.process(img2)  # 開始偵測全身\n",
        "            mp_drawing.draw_landmarks(\n",
        "                img,\n",
        "                results.pose_landmarks,\n",
        "                mp_holistic.POSE_CONNECTIONS,\n",
        "                landmark_drawing_spec=mp_drawing_styles\n",
        "                .get_default_pose_landmarks_style())\n",
        "\n",
        "            # Overwrite the original image\n",
        "            final[y:y + h, x:x + w] = img\n",
        "            # cv2.imshow(\"a\", final)\n",
        "            cv2.imwrite(\"sample_data/gif/\"+str(count)+\".jpg\", final)\n",
        "            count += 1\n",
        "            end = time.time()\n",
        "            # 輸出結果\n",
        "            print(\"mediapipe執行時間：%f 秒\" % (end - start))\n",
        "\n",
        "            # get erery pose_landmarks and transform to original img loc\n",
        "            # landmark_list = results.pose_landmarks\n",
        "            # image_rows, image_cols, _ = frame.shape\n",
        "            # idx_to_coordinates = {}\n",
        "\n",
        "            # if landmark_list != None:\n",
        "            #     for idx, landmark in enumerate(landmark_list.landmark):\n",
        "            #         if ((landmark.HasField('visibility') and\n",
        "            #              landmark.visibility < _VISIBILITY_THRESHOLD) or\n",
        "            #                 (landmark.HasField('presence') and\n",
        "            #                  landmark.presence < _PRESENCE_THRESHOLD)):\n",
        "            #             continue\n",
        "            #         landmark_px = _normalized_to_pixel_coordinates(landmark.x, landmark.y, image_cols, image_rows)\n",
        "            #         if landmark_px:\n",
        "            #             idx_to_coordinates[idx] = landmark_px\n",
        "            #     if len(idx_to_coordinates) != 0:\n",
        "            #         mp_array.append(idx_to_coordinates)\n",
        "\n",
        "            #         # transform to original loc\n",
        "            #         for j in idx_to_coordinates:\n",
        "            #             # original x\n",
        "            #             # print(x + idx_to_coordinates.get(j)[0])\n",
        "            #             # original y\n",
        "            #             # print(y + idx_to_coordinates.get(j)[0])\n",
        "            #             print()\n",
        "\n",
        "                    # Because some points cannot be determined,\n",
        "                    # the number of each point has a corresponding position\n",
        "                    # cannot append to array.\n",
        "        # print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Release the video capture object and close any open windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_80PT-JImzbg",
        "outputId": "303d9e3e-326b-475e-fc15-ab1b6c7475ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 235.2ms\n",
            "Speed: 3.3ms preprocess, 235.2ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：10.232624 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 9.2ms\n",
            "Speed: 3.5ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：10.570589 秒\n",
            "mediapipe執行時間：10.732665 秒\n",
            "yolo執行時間：0.032544 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.6ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.125710 秒\n",
            "mediapipe執行時間：0.212806 秒\n",
            "yolo執行時間：0.020695 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.123896 秒\n",
            "mediapipe執行時間：0.227525 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 2 persons, 63.0ms\n",
            "Speed: 8.5ms preprocess, 63.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.104223 秒\n",
            "mediapipe執行時間：0.352644 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 13.2ms\n",
            "Speed: 2.8ms preprocess, 13.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.565834 秒\n",
            "yolo執行時間：0.032210 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.212911 秒\n",
            "mediapipe執行時間：0.371039 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 2 persons, 15.6ms\n",
            "Speed: 12.4ms preprocess, 15.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.067884 秒\n",
            "mediapipe執行時間：0.226781 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.315188 秒\n",
            "yolo執行時間：0.023865 秒\n",
            "mediapipe執行時間：0.110980 秒\n",
            "mediapipe執行時間：0.200201 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.023296 秒\n",
            "mediapipe執行時間：0.122027 秒\n",
            "mediapipe執行時間：0.209267 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 2 persons, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.022950 秒\n",
            "mediapipe執行時間：0.115271 秒\n",
            "mediapipe執行時間：0.199083 秒\n",
            "yolo執行時間：0.019441 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.111092 秒\n",
            "mediapipe執行時間：0.192991 秒\n",
            "yolo執行時間：0.020216 秒\n",
            "mediapipe執行時間：0.104788 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 5.1ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.190016 秒\n",
            "yolo執行時間：0.025307 秒\n",
            "mediapipe執行時間：0.109470 秒\n",
            "mediapipe執行時間：0.193774 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.026626 秒\n",
            "mediapipe執行時間：0.112414 秒\n",
            "mediapipe執行時間：0.209928 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 2 persons, 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.024245 秒\n",
            "mediapipe執行時間：0.113097 秒\n",
            "mediapipe執行時間：0.199228 秒\n",
            "yolo執行時間：0.020375 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 10.6ms\n",
            "Speed: 3.3ms preprocess, 10.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.114854 秒\n",
            "mediapipe執行時間：0.199169 秒\n",
            "yolo執行時間：0.023905 秒\n",
            "mediapipe執行時間：0.114331 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 3 persons, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.204465 秒\n",
            "yolo執行時間：0.021775 秒\n",
            "mediapipe執行時間：0.113113 秒\n",
            "mediapipe執行時間：0.196475 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 4.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.241131 秒\n",
            "yolo執行時間：0.020766 秒\n",
            "mediapipe執行時間：0.147247 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.232635 秒\n",
            "yolo執行時間：0.023533 秒\n",
            "mediapipe執行時間：0.115474 秒\n",
            "mediapipe執行時間：0.199973 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo執行時間：0.022281 秒\n",
            "mediapipe執行時間：0.108896 秒\n",
            "mediapipe執行時間：0.189551 秒\n",
            "yolo執行時間：0.023292 秒\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 5.2ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediapipe執行時間：0.107700 秒\n",
            "mediapipe執行時間：0.196882 秒\n",
            "yolo執行時間：0.021761 秒\n",
            "mediapipe執行時間：0.127390 秒\n",
            "mediapipe執行時間：0.217202 秒\n"
          ]
        }
      ]
    }
  ]
}
